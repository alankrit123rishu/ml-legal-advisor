{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1JhPv4ddSVT0DKybrXykQgk7A5A5xikRY","authorship_tag":"ABX9TyOUMGF02CECF7c2F68Ise4a"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r1wu9FeA-sFa","executionInfo":{"status":"ok","timestamp":1742550317548,"user_tz":-330,"elapsed":89080,"user":{"displayName":"Alankrit Tomar","userId":"08857353945656133629"}},"outputId":"01201bbd-3fb6-465f-a133-035ebf531375"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.49.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.29.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n","Current working directory: /content\n","Data sample:\n","                        _id      BNS_Section                 IPC_Section  \\\n","0  67c8bccfdcd2f83aa7d0f449   Sec. 1 (1)-(6)                 S. 1 - S. 5   \n","1  67c8bccfdcd2f83aa7d0f44a  Sec. 2 (1)-(39)               S. 6 - S. 52A   \n","2  67c8bccfdcd2f83aa7d0f44b   Sec. 3 (1)-(9)  S. 6, 7, 27, 32, 34, 35-38   \n","3  67c8bccfdcd2f83aa7d0f44c           Sec. 4                       S. 53   \n","4  67c8bccfdcd2f83aa7d0f44d           Sec. 5              S. 54 - S. 55A   \n","\n","                      Chapter  \\\n","0      Chapter I: Preliminary   \n","1      Chapter I: Preliminary   \n","2      Chapter I: Preliminary   \n","3  Chapter II: Of Punishments   \n","4  Chapter II: Of Punishments   \n","\n","                                         Description  \n","0  This section outlines the basic framework of t...  \n","1  This section provides definitions for key term...  \n","2  This section offers general explanations for c...  \n","3  This section details punishments under the BNS...  \n","4  This section governs the commutation of senten...  \n","Grouping by: IPC_Section\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Training on cpu for 3 epochs...\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/3: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s, loss=0.0588]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3 completed. Average loss: 0.2218\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/3: 100%|██████████| 13/13 [00:08<00:00,  1.60it/s, loss=0.2140]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/3 completed. Average loss: 0.2445\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/3: 100%|██████████| 13/13 [00:07<00:00,  1.80it/s, loss=0.3114]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/3 completed. Average loss: 0.2340\n","Model training complete. The fine-tuned model is saved to 'fine_tuned_model'.\n"]}],"source":["!pip install sentence-transformers\n","\n","import pandas as pd\n","import random\n","import torch\n","import numpy as np\n","from sentence_transformers import SentenceTransformer, util\n","from tqdm import tqdm\n","import os\n","\n","# ------------------------------\n","# Load CSV data\n","# ------------------------------\n","# Get the current working directory\n","current_directory = os.getcwd()\n","print(\"Current working directory:\", current_directory)\n","\n","# Construct the relative path to the data file\n","\n","data_file = os.path.join(current_directory, 'mllegaladvisordb.bns_sectionsnew.csv')\n","# or use absolute path if the file is in a different location\n","# data_file = r\"C:\\Users\\Admin\\Desktop\\mllegaladvisordb.bns_sectionsnew.csv\"\n","\n","# Check if the file exists\n","if not os.path.exists(data_file):\n","    raise FileNotFoundError(f\"The file '{data_file}' does not exist.\")\n","\n","df = pd.read_csv(data_file)\n","\n","print(\"Data sample:\")\n","print(df.head())\n","\n","# Ensure required columns exist\n","required_columns = ['BNS_Section', 'IPC_Section', 'Description']\n","for col in required_columns:\n","    if col not in df.columns:\n","        raise ValueError(f\"Missing required column: {col}\")\n","\n","# ------------------------------\n","# Decide grouping column based on data distribution\n","# ------------------------------\n","if df['BNS_Section'].nunique() < df.shape[0] / 2:\n","    group_column = 'BNS_Section'\n","else:\n","    group_column = 'IPC_Section'\n","print(f\"Grouping by: {group_column}\")\n","\n","# ------------------------------\n","# Prepare data in a simpler way\n","# ------------------------------\n","# Collect all descriptions and their groups\n","descriptions = []\n","groups = []\n","for _, row in df.iterrows():\n","    descriptions.append(row['Description'])\n","    groups.append(row[group_column])\n","\n","# ------------------------------\n","# Load Pre-trained SentenceTransformer Model\n","# ------------------------------\n","model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# ------------------------------\n","# Manual training loop using triplet loss approach\n","# ------------------------------\n","# Training parameters\n","num_epochs = 3\n","learning_rate = 2e-5\n","batch_size = 8  # Smaller batch size to avoid memory issues\n","margin = 0.5  # Margin for triplet loss\n","\n","# Setup optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Output path\n","output_path = 'fine_tuned_model'\n","\n","print(f\"Training on {device} for {num_epochs} epochs...\")\n","\n","for epoch in range(num_epochs):\n","    # Create triplets for this epoch\n","    triplets = []\n","\n","    # Group descriptions by their group\n","    group_to_descriptions = {}\n","    for desc, grp in zip(descriptions, groups):\n","        if grp not in group_to_descriptions:\n","            group_to_descriptions[grp] = []\n","        group_to_descriptions[grp].append(desc)\n","\n","    # Only use groups with at least 2 descriptions\n","    valid_groups = [g for g, descs in group_to_descriptions.items() if len(descs) >= 2]\n","\n","    # Create triplets: (anchor, positive, negative)\n","    # Where positive is from same group, negative from different group\n","    for _ in range(100):  # Generate a fixed number of triplets per epoch\n","        # Select a random group that has at least 2 items\n","        if not valid_groups:\n","            print(\"No valid groups found with at least 2 descriptions\")\n","            break\n","\n","        anchor_group = random.choice(valid_groups)\n","\n","        # Select anchor and positive from the same group\n","        anchor, positive = random.sample(group_to_descriptions[anchor_group], 2)\n","\n","        # Select a different group for negative\n","        negative_groups = [g for g in valid_groups if g != anchor_group]\n","        if not negative_groups:\n","            print(\"No different groups available for negative samples\")\n","            continue\n","\n","        negative_group = random.choice(negative_groups)\n","        negative = random.choice(group_to_descriptions[negative_group])\n","\n","        triplets.append((anchor, positive, negative))\n","\n","    if not triplets:\n","        print(\"No triplets could be created. Skipping epoch.\")\n","        continue\n","\n","    # Train on triplets\n","    model.train()\n","    train_loss = 0\n","\n","    # Process in batches\n","    random.shuffle(triplets)\n","    num_batches = len(triplets) // batch_size + (1 if len(triplets) % batch_size > 0 else 0)\n","\n","    progress_bar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n","\n","    for i in progress_bar:\n","        start_idx = i * batch_size\n","        end_idx = min((i + 1) * batch_size, len(triplets))\n","        batch_triplets = triplets[start_idx:end_idx]\n","\n","        # Extract anchors, positives, and negatives\n","        anchors = [t[0] for t in batch_triplets]\n","        positives = [t[1] for t in batch_triplets]\n","        negatives = [t[2] for t in batch_triplets]\n","\n","        # Reset gradients\n","        optimizer.zero_grad()\n","\n","        # Get embeddings\n","        anchor_embeddings = model.encode(anchors, convert_to_tensor=True, device=device)\n","        positive_embeddings = model.encode(positives, convert_to_tensor=True, device=device)\n","        negative_embeddings = model.encode(negatives, convert_to_tensor=True, device=device)\n","\n","        # Calculate distances\n","        positive_distances = 1 - util.pytorch_cos_sim(anchor_embeddings, positive_embeddings).diagonal()\n","        negative_distances = 1 - util.pytorch_cos_sim(anchor_embeddings, negative_embeddings).diagonal()\n","\n","        # Ensure distances require gradients\n","        positive_distances = positive_distances.requires_grad_()\n","        negative_distances = negative_distances.requires_grad_()\n","\n","        # Triplet loss\n","        losses = torch.relu(positive_distances - negative_distances + margin)\n","        loss = torch.mean(losses)\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update stats\n","        train_loss += loss.item()\n","        progress_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n","\n","    # Epoch summary\n","    avg_loss = train_loss / num_batches\n","    print(f\"Epoch {epoch+1}/{num_epochs} completed. Average loss: {avg_loss:.4f}\")\n","\n","# Save the model\n","model.save(output_path)\n","print(f\"Model training complete. The fine-tuned model is saved to '{output_path}'.\")"]},{"cell_type":"code","source":["from google.colab import files\n","!zip -r /content/fine_tuned_model.zip /content/fine_tuned_model\n","files.download('/content/fine_tuned_model.zip')"],"metadata":{"id":"8aE9dDVnI-CJ","executionInfo":{"status":"ok","timestamp":1742551125606,"user_tz":-330,"elapsed":8169,"user":{"displayName":"Alankrit Tomar","userId":"08857353945656133629"}},"outputId":"594cf50a-5229-439c-adfd-93b863c81146","colab":{"base_uri":"https://localhost:8080/","height":295}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/fine_tuned_model/ (stored 0%)\n","  adding: content/fine_tuned_model/model.safetensors (deflated 9%)\n","  adding: content/fine_tuned_model/2_Normalize/ (stored 0%)\n","  adding: content/fine_tuned_model/config.json (deflated 47%)\n","  adding: content/fine_tuned_model/modules.json (deflated 62%)\n","  adding: content/fine_tuned_model/1_Pooling/ (stored 0%)\n","  adding: content/fine_tuned_model/1_Pooling/config.json (deflated 57%)\n","  adding: content/fine_tuned_model/config_sentence_transformers.json (deflated 34%)\n","  adding: content/fine_tuned_model/README.md (deflated 64%)\n","  adding: content/fine_tuned_model/tokenizer.json (deflated 71%)\n","  adding: content/fine_tuned_model/sentence_bert_config.json (deflated 4%)\n","  adding: content/fine_tuned_model/vocab.txt (deflated 53%)\n","  adding: content/fine_tuned_model/.ipynb_checkpoints/ (stored 0%)\n","  adding: content/fine_tuned_model/tokenizer_config.json (deflated 73%)\n","  adding: content/fine_tuned_model/special_tokens_map.json (deflated 80%)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_114e6031-9a9b-484d-87a1-6958f5f60b22\", \"fine_tuned_model.zip\", 83449409)"]},"metadata":{}}]},{"cell_type":"markdown","source":["# New section"],"metadata":{"id":"bwA24oIbBQPa"}}]}